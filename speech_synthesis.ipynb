{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from: https://github.com/dgaddy/silent_speech\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append('./hifi_gan')\n",
    "from env import AttrDict\n",
    "from models import Generator\n",
    "\n",
    "# from absl import flags\n",
    "# FLAGS = flags.FLAGS\n",
    "# flags.DEFINE_string('hifigan_checkpoint', None, 'filename of hifi-gan generator checkpoint')\n",
    "\n",
    "class Vocoder(object):\n",
    "    def __init__(self, hifigan_checkpoint=None, device='cuda'):\n",
    "        assert hifigan_checkpoint is not None\n",
    "        checkpoint_file = hifigan_checkpoint\n",
    "        config_file = os.path.join(os.path.split(checkpoint_file)[0], 'config.json')\n",
    "        with open(config_file) as f:\n",
    "            hparams = AttrDict(json.load(f))\n",
    "        self.generator = Generator(hparams).to(device)\n",
    "        self.generator.load_state_dict(torch.load(checkpoint_file)['generator'])\n",
    "        self.generator.eval()\n",
    "        self.generator.remove_weight_norm()\n",
    "\n",
    "    def __call__(self, mel_spectrogram):\n",
    "        '''\n",
    "            mel_spectrogram should be a tensor of shape (seq_len, 80)\n",
    "            returns 1d tensor of audio\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            mel_spectrogram = mel_spectrogram.T[np.newaxis,:,:]\n",
    "            audio = self.generator(mel_spectrogram)\n",
    "        return audio.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "vocoder = Vocoder(\"./hifigan_finetuned/checkpoint\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_mel_spectrograms(pred, text):\n",
    "    fig, ax = plt.subplots(1) # nrows=1, ncols=2)\n",
    "\n",
    "    # ax[0].set_title(f\"Mel Spectogram (Predicted)\")\n",
    "    pred = np.swapaxes(pred, 0, 1)\n",
    "    cax = ax.imshow(pred, interpolation='nearest', cmap=cm.coolwarm, origin='lower')\n",
    "\n",
    "    ax.set_title(text)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis Ground Truth JP Shorts Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/joe/DCC4B54EC4B52C20/Users/win8t/OneDrive/Desktop/projects/uni-all/comp-vis-avhubert/lib/__init__.py:57: FutureWarning: Pass y=[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.78460839e-04\n",
      " -4.07649505e-05  0.00000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio_features = librosa.feature.melspectrogram(\n"
     ]
    }
   ],
   "source": [
    "from lib import load_audio, get_audio_feats\n",
    "\n",
    "test_audio_path = \"./dataset/wsDmwoOrpR8/The False Appeal of Communism.mp3\"\n",
    "\n",
    "audio_arr   = load_audio(test_audio_path)\n",
    "audio_feats = get_audio_feats(audio_arr, n_mel_channels=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5136, 80)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_feats = np.expand_dims(audio_feats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feats = torch.tensor(audio_feats).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_audio = vocoder(audio_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1314816])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sf.write('pred_output.wav', pred_audio.cpu().numpy(), 16_000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis Mel Spectrogram Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feats = torch.load(\"./overfit_lecture_speech_features_melchannel80.pt\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(audio_feats.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12404/4054834838.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio_feats = torch.tensor(audio_feats).to(\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "# audio_feats = torch.tensor(audio_feats).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20506, 80])\n"
     ]
    }
   ],
   "source": [
    "print(audio_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feats_np = audio_feats.detach().cpu().numpy()\n",
    "#fig, ax = plot_mel_spectrograms(audio_feats_np, \"Pre-Interpolated Mel Spectrogram\")\n",
    "##plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate Predicted Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "\n",
    "def interpolate_spectrogram(spectrogram, original_hop_length, target_hop_length):\n",
    "    original_time_steps, num_freqs = spectrogram.shape\n",
    "\n",
    "    # Calculate the number of time steps for the target hop length\n",
    "    target_time_steps = int(original_time_steps * (original_hop_length / target_hop_length))\n",
    "\n",
    "    # Define a function for the interpolation\n",
    "    x = np.arange(original_time_steps)\n",
    "    f = interpolate.interp1d(x, spectrogram, axis=0, kind='linear')\n",
    "\n",
    "    # Generate the new time steps\n",
    "    x_new = np.linspace(0, original_time_steps-1, target_time_steps)\n",
    "\n",
    "    # Apply the interpolation function\n",
    "    new_spectrogram = f(x_new)\n",
    "\n",
    "    return new_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_audio_feats = interpolate_spectrogram(\n",
    "    audio_feats_np,\n",
    "    original_hop_length=534,\n",
    "    target_hop_length=int(534//2))\n",
    "    #target_hop_length=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41012, 80)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stretched_audio_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_audio_feats = torch.tensor(stretched_audio_feats).float().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_audio_feats_20pc = final_audio_feats[:int(final_audio_feats.shape[0] * 0.2), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8202, 80])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_audio_feats_20pc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_audio = vocoder(final_audio_feats_20pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sf.write('pred_output.wav', pred_audio.cpu().numpy(), 16_000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What are your topics of strange things? The first thing is, when we've done you know the specifics of the digital thing program, it's different to impact on them. It's had a particular impact on where I'm called excluded them. And so that would be no less to this, the point already, or majority of men who are giving their wealth. So for example, at Warholkart, the digital thing program had a particular more positive impact on Warholkart students who are then around them very well in high school. And I'm pretty amazed you would have a clear destination out there. So you know those people are a big industry relationship with the idea of education. And they're not only specifically towards people, they're not very good at it. Now, why do they have a different tool effect from men to be questioned? Well, first of all, what we're going to do is go, so right just the amount of the fact that it does better for people who are doing as well. And after all, it's the most of them in that. I don't really know. I don't really know when we're doing that. It's completely agreeable. And so the system sets out the structure and says, here's a pathway to attainment, for women who are going and stuff, they're going on with, and that's what you do if all for them. And especially the way they're on the disagreeable end of the distribution. And it's very normal now on the disagreeable end of the distribution then there are women. But then, there are a lot of the issues. So very, very, very, very, very less of the distribution. And then they talk about it. People are learning. There's a lot of people are learning. And meaning the real difference is no current in exchange. So it's a very interesting sign effect on overarching distributions. So I mean, people can be mostly the same, but that can still produce radical differences. Disagreeable men won't do anything they don't want to do. They can say, up years, how the whole world would play new games. You know, there are those existing classes. And why should I look for you? I'll just go ahead.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"pred_output.wav\")\n",
    "print(result[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
