{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from: https://github.com/dgaddy/silent_speech\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append('./hifi_gan')\n",
    "from env import AttrDict\n",
    "from models import Generator\n",
    "\n",
    "# from absl import flags\n",
    "# FLAGS = flags.FLAGS\n",
    "# flags.DEFINE_string('hifigan_checkpoint', None, 'filename of hifi-gan generator checkpoint')\n",
    "\n",
    "class Vocoder(object):\n",
    "    def __init__(self, hifigan_checkpoint=None, device='cuda'):\n",
    "        assert hifigan_checkpoint is not None\n",
    "        checkpoint_file = hifigan_checkpoint\n",
    "        config_file = os.path.join(os.path.split(checkpoint_file)[0], 'config.json')\n",
    "        with open(config_file) as f:\n",
    "            hparams = AttrDict(json.load(f))\n",
    "        self.generator = Generator(hparams).to(device)\n",
    "        self.generator.load_state_dict(torch.load(checkpoint_file)['generator'])\n",
    "        self.generator.eval()\n",
    "        self.generator.remove_weight_norm()\n",
    "\n",
    "    def __call__(self, mel_spectrogram):\n",
    "        '''\n",
    "            mel_spectrogram should be a tensor of shape (seq_len, 80)\n",
    "            returns 1d tensor of audio\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            mel_spectrogram = mel_spectrogram.T[np.newaxis,:,:]\n",
    "            audio = self.generator(mel_spectrogram)\n",
    "        return audio.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "vocoder = Vocoder(\"./hifigan_finetuned/checkpoint\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_mel_spectrograms(pred, text):\n",
    "    fig, ax = plt.subplots(1) # nrows=1, ncols=2)\n",
    "\n",
    "    # ax[0].set_title(f\"Mel Spectogram (Predicted)\")\n",
    "    pred = np.swapaxes(pred, 0, 1)\n",
    "    cax = ax.imshow(pred, interpolation='nearest', cmap=cm.coolwarm, origin='lower')\n",
    "\n",
    "    ax.set_title(text)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis Ground Truth JP Shorts Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "/media/joe/DCC4B54EC4B52C20/Users/win8t/OneDrive/Desktop/projects/uni-all/comp-vis-avhubert/lib/__init__.py:57: FutureWarning: Pass y=[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.78460839e-04\n",
      " -4.07649505e-05  0.00000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio_features = librosa.feature.melspectrogram(\n"
     ]
    }
   ],
   "source": [
    "from lib import load_audio, get_audio_feats\n",
    "\n",
    "test_audio_path = \"./dataset/wsDmwoOrpR8/The False Appeal of Communism.mp3\"\n",
    "\n",
    "audio_arr   = load_audio(test_audio_path)\n",
    "audio_feats = get_audio_feats(audio_arr, n_mel_channels=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5136, 80)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_feats = np.expand_dims(audio_feats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feats = torch.tensor(audio_feats).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_audio = vocoder(audio_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1314816])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sf.write('pred_output.wav', pred_audio.cpu().numpy(), 16_000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis Mel Spectrogram Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_feats = torch.load(\"./overfit_lecture_speech_features_melchannel80.pt\").to(\"cpu\")\n",
    "#] audio_feats = torch.load(\"./general_lecture_speech_features_melchannel80.pt\").to(\"cpu\")\n",
    "# audio_feats = torch.load(\"./validate_valid_lecture_speech_features_melchannel80.pt\").to(\"cpu\")\n",
    "audio_feats = torch.load(\"./full_model_valid_mel_pred.pt\").to(\"cpu\")\n",
    "# audio_feats = torch.load(\"./full_model_valid_mel_ground.pt\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_mel_spectrograms(audio_feats, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(audio_feats.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win8t\\AppData\\Local\\Temp/ipykernel_19584/4054834838.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio_feats = torch.tensor(audio_feats).to(\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "audio_feats = torch.tensor(audio_feats).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2078, 80])\n"
     ]
    }
   ],
   "source": [
    "print(audio_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feats_np = audio_feats.detach().cpu().numpy()\n",
    "#fig, ax = plot_mel_spectrograms(audio_feats_np, \"Pre-Interpolated Mel Spectrogram\")\n",
    "##plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate Predicted Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "\n",
    "def interpolate_spectrogram(spectrogram, original_hop_length, target_hop_length):\n",
    "    original_time_steps, num_freqs = spectrogram.shape\n",
    "\n",
    "    # Calculate the number of time steps for the target hop length\n",
    "    target_time_steps = int(original_time_steps * (original_hop_length / target_hop_length))\n",
    "\n",
    "    # Define a function for the interpolation\n",
    "    x = np.arange(original_time_steps)\n",
    "    f = interpolate.interp1d(x, spectrogram, axis=0, kind='linear')\n",
    "\n",
    "    # Generate the new time steps\n",
    "    x_new = np.linspace(0, original_time_steps-1, target_time_steps)\n",
    "\n",
    "    # Apply the interpolation function\n",
    "    new_spectrogram = f(x_new)\n",
    "\n",
    "    return new_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_audio_feats = interpolate_spectrogram(\n",
    "    audio_feats_np,\n",
    "    original_hop_length=534,\n",
    "    target_hop_length=int(534//2))\n",
    "    # target_hop_length=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4156, 80)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stretched_audio_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_audio_feats = torch.tensor(stretched_audio_feats).float().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_audio_feats_10pc = final_audio_feats[:int(final_audio_feats.shape[0] * 0.10), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([415, 80])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_audio_feats_10pc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4156, 80])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_audio_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_audio = vocoder(final_audio_feats_10pc)\n",
    "# pred_audio = vocoder(final_audio_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sf.write('pred_output.wav', pred_audio.cpu().numpy(), 16_000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you should quit πάocrons or the Street Times, so that all these Citocrons are changed. Well, I did quite a lot from your perspective.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"pred_output.wav\")\n",
    "print(result[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
